{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, the user asked about LangChain. I remember they're related to natural \"\n",
      " 'language processing.\\n'\n",
      " '\\n'\n",
      " \"LangChain combines PyTorch with LLMs and NLP models like Sentence-BERT. It's \"\n",
      " 'used for tasks like chatbots and text generation.\\n'\n",
      " '\\n'\n",
      " 'I should explain its core components—PyTorch, LLMs, and how it works '\n",
      " 'internally.\\n'\n",
      " '\\n'\n",
      " 'Maybe mention the pipeline: data transformation through transformers and '\n",
      " 'then sentinels. Also, list use cases briefly.\\n'\n",
      " '\\n'\n",
      " \"Make sure to keep it clear and concise since they're new to this topic.\\n\"\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is an open-source Python library that integrates PyTorch deep '\n",
      " 'learning with large language models (LLMs) to perform tasks related to '\n",
      " 'natural language processing. It provides a high-level interface for building '\n",
      " 'chatbots, text generation systems, and other NLP applications.\\n'\n",
      " '\\n'\n",
      " '### Core Components of LangChain:\\n'\n",
      " '1. **PyTorch Integration**: LangChain leverages the power of PyTorch for '\n",
      " 'efficient computation and flexibility.\\n'\n",
      " '2. **LLMs (Large Language Models)**: It uses LLMs, such as Sentence-BERT or '\n",
      " 'BERT, to generate text responses or perform other NLP tasks.\\n'\n",
      " '3. **Pipeline Framework**: The library provides a pipeline for building '\n",
      " 'systems that combine text generation with the generation of user questions '\n",
      " 'or prompts.\\n'\n",
      " '\\n'\n",
      " '### How LangChain Works:\\n'\n",
      " 'LangChain works by defining a series of components within a PyTorch-based '\n",
      " 'framework. These components include:\\n'\n",
      " '- **Preprocessing and Text Generation**: Generating responses to user '\n",
      " 'queries.\\n'\n",
      " '- **Text Generation**: Using LLMs to produce natural language outputs, such '\n",
      " 'as chat completions or text generation for applications like chatbots or '\n",
      " 'content creation.\\n'\n",
      " '- **Question and Prompt Generation**: Creating questions that guide the '\n",
      " 'system towards specific outcomes.\\n'\n",
      " '\\n'\n",
      " '### Use Cases:\\n'\n",
      " 'LangChain is used in various NLP tasks, including:\\n'\n",
      " '- **Chatbots**: Building systems that can interact with users by answering '\n",
      " 'questions.\\n'\n",
      " '- **Text Generation**: Generating text from prompts, such as writing '\n",
      " 'proposals or writing descriptions of products.\\n'\n",
      " '- **Content Creation**: Automating the creation of content based on user '\n",
      " 'input.\\n'\n",
      " '\\n'\n",
      " '### Example Workflow:\\n'\n",
      " '1. Define a prompt to get information about a topic.\\n'\n",
      " '2. Use LangChain to process this prompt and generate an answer.\\n'\n",
      " \"3. Use the system's response to form a new question that asks for additional \"\n",
      " 'details.\\n'\n",
      " '\\n'\n",
      " '### Summary:\\n'\n",
      " 'LangChain is a powerful tool for building systems that require natural '\n",
      " 'language processing, particularly those involving AI-driven responses or '\n",
      " 'automated content creation. It combines PyTorch with LLMs to provide '\n",
      " 'flexible and efficient solutions for various NLP tasks.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 deepseek-r1:1.5b 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it's open-source and has a large community. Also, it's used in various fields like web development, data analysis, artificial intelligence, and more.\n",
      "\n",
      "Wait, the user might be a beginner, so I should keep the explanation simple. Maybe start with the basic definition, then touch on its features and applications. Oh, and include that it's written in Python syntax, which is easy to read. Also, mention that it's cross-platform, so it works on different operating systems. \n",
      "\n",
      "I should make sure not to use any technical jargon that's too complex. Maybe list some key points like being free, having a vast ecosystem, and being used in both small and large projects. Also, note that it's interpreted, which means it's not compiled like C++. \n",
      "\n",
      "Oh, and maybe add a sentence about how it's popular because of its simplicity and the extensive libraries available. That would make the answer more comprehensive. Let me check if there's any other important aspect I should include. Maybe the fact that it's used in data science with libraries like Pandas and NumPy. \n",
      "\n",
      "Wait, the user didn't specify the level of detail, so keeping it concise but informative is best. Alright, structure the answer with a clear definition, key features, and applications. Make sure it's in Korean and flows naturally.\n",
      "</think>\n",
      "\n",
      "파이썬은 **프로그래밍 언어**입니다.  \n",
      "그것은 간단하고 직관적인 문법으로, 다양한 분야에서 사용되며, 웹 개발, 데이터 분석, 인공지능, 앱 개발 등 다양한 분야에 넓게 활용되고 있습니다.  \n",
      "파이썬은 오픈소스이며, 커뮤니티가 활발하여 많은 라이브러리와 도구가 존재합니다.  \n",
      "또, 코딩이 쉬워서 초보자도 쉽게 시작할 수 있어요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 qwen2.5:1.5b 모델 로드\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한글로 답변해 줘\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
      "\n",
      "To make a fair comparison, it's helpful to have the same number of decimal places for both numbers. \n",
      "\n",
      "I can rewrite 9.9 as 9.90 by adding a zero in the hundredths place. This way, both numbers will have two decimal places: 9.90 and 9.11.\n",
      "\n",
      "Now, I'll compare each corresponding digit from left to right.\n",
      "\n",
      "Starting with the units place, both numbers have a 9, so they are equal here.\n",
      "\n",
      "Next, in the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Align the Decimal Places\n",
      "First, it's helpful to write both numbers with the same number of decimal places for a clear comparison.\n",
      "\n",
      "- **9.9** can be written as **9.90**\n",
      "- **9.11** remains **9.11**\n",
      "\n",
      "### Step 2: Compare Digit by Digit\n",
      "Now, compare each corresponding digit from left to right:\n",
      "\n",
      "1. **Units Place:**  \n",
      "   Both numbers have a **9** in the units place.\n",
      "\n",
      "2. **Tenths Place:**  \n",
      "   - **9.90** has a **9** in the tenths place.\n",
      "   - **9.11** has a **1** in the tenths place.\n",
      "\n",
      "Since **9 > 1**, **9.90** is larger than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "Therefore, **9.9** is bigger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
       "\n",
       "To make a fair comparison, it's helpful to have the same number of decimal places for both numbers. \n",
       "\n",
       "I can rewrite 9.9 as 9.90 by adding a zero in the hundredths place. This way, both numbers will have two decimal places: 9.90 and 9.11.\n",
       "\n",
       "Now, I'll compare each corresponding digit from left to right.\n",
       "\n",
       "Starting with the units place, both numbers have a 9, so they are equal here.\n",
       "\n",
       "Next, in the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Align the Decimal Places\n",
       "First, it's helpful to write both numbers with the same number of decimal places for a clear comparison.\n",
       "\n",
       "- **9.9** can be written as **9.90**\n",
       "- **9.11** remains **9.11**\n",
       "\n",
       "### Step 2: Compare Digit by Digit\n",
       "Now, compare each corresponding digit from left to right:\n",
       "\n",
       "1. **Units Place:**  \n",
       "   Both numbers have a **9** in the units place.\n",
       "\n",
       "2. **Tenths Place:**  \n",
       "   - **9.90** has a **9** in the tenths place.\n",
       "   - **9.11** has a **1** in the tenths place.\n",
       "\n",
       "Since **9 > 1**, **9.90** is larger than **9.11**.\n",
       "\n",
       "### Conclusion\n",
       "Therefore, **9.9** is bigger than **9.11**.\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is definitely bigger than 0.11. Therefore, even though both numbers have the same whole number part, the decimal part of 9.9 is larger, making 9.9 bigger than 9.11.\n",
      "\n",
      "Let me double-check. If I write them out: 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So the answer should be 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다.  \n",
      "**이유:**  \n",
      "- 두 수 모두 9를 기준으로 하며, 소수점 부분을 비교해야 합니다.  \n",
      "- 9.9는 0.9를, 9.11는 0.11을 소수점 부분으로 가집니다.  \n",
      "- 0.9는 0.11보다 크므로, 9.9 > 9.11입니다.  \n",
      "\n",
      "**답변:** 9.9가 더 큰 수입니다."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3623342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
       "\n",
       "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
       "\n",
       "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is definitely bigger than 0.11. Therefore, even though both numbers have the same whole number part, the decimal part of 9.9 is larger, making 9.9 bigger than 9.11.\n",
       "\n",
       "Let me double-check. If I write them out: 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So the answer should be 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "9.9는 9.11보다 더 큰 수입니다.  \n",
       "**이유:**  \n",
       "- 두 수 모두 9를 기준으로 하며, 소수점 부분을 비교해야 합니다.  \n",
       "- 9.9는 0.9를, 9.11는 0.11을 소수점 부분으로 가집니다.  \n",
       "- 0.9는 0.11보다 크므로, 9.9 > 9.11입니다.  \n",
       "\n",
       "**답변:** 9.9가 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraph를 사용하여 DeepSeek 모델(추론)과 Qwen 모델(한글응답)을 연동하기\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f146748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.11**입니다.  \n",
      "두 수는 정수 부분(9)이 동일하므로, 소수 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 바꾸면, 9.11은 9.11로 보입니다.  \n",
      "소수점 아래 첫 번째 자리(십분의 1)에서는 둘 다 9이므로, 두 번째 자리(백분의 1)에서 비교해야 합니다.  \n",
      "9.9는 0을, 9.11은 1을 가집니다. 1은 0보다 큰 수이므로, 9.11이 더 커집니다.  \n",
      "\n",
      "따라서, **9.11이 더 큰 수입니다**.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit after the decimal point:\\n\\n- The tenths place: Both numbers have 9.\\n- The hundredths place: 9 has 0, while 1.11 has 1.\\n\\nSince 1 is greater than 0 in the hundredths place, 9.11 is larger than 9.90.\\n\\nTherefore, 9.11 is greater than 9.9.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\\n\\nI remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\\n</think>\\n\\n9.9와 9.11 중 더 큰 수는 **9.11**입니다.  \\n두 수는 정수 부분(9)이 동일하므로, 소수 부분을 비교해야 합니다.  \\n9.9는 9.90으로 바꾸면, 9.11은 9.11로 보입니다.  \\n소수점 아래 첫 번째 자리(십분의 1)에서는 둘 다 9이므로, 두 번째 자리(백분의 1)에서 비교해야 합니다.  \\n9.9는 0을, 9.11은 1을 가집니다. 1은 0보다 큰 수이므로, 9.11이 더 커집니다.  \\n\\n따라서, **9.11이 더 큰 수입니다**.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.11**입니다.  \n",
      "두 수는 정수 부분(9)이 동일하므로, 소수 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 바꾸면, 9.11은 9.11로 보입니다.  \n",
      "소수점 아래 첫 번째 자리(십분의 1)에서는 둘 다 9이므로, 두 번째 자리(백분의 1)에서 비교해야 합니다.  \n",
      "9.9는 0을, 9.11은 1을 가집니다. 1은 0보다 큰 수이므로, 9.11이 더 커집니다.  \n",
      "\n",
      "따라서, **9.11이 더 큰 수입니다**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 추론 모델\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#응답 모델 (한글처리 가능)\n",
    "#generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "#qwen3:1.7b\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6834b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG0AAAFNCAIAAACFQXaDAAAAAXNSR0IArs4c6QAAG8tJREFUeJztnXdgU9X+wE920qzukQ7aQqEtbZoOQJDHLkNkK6OALEVARJ4UGTJFnzL04fsJigxFRKk8hlKWVjbUQqGTyurebdpmr3tv8vsjWPsgTdL0pEngfP5K7j333G8/Pffek3vPPV+SwWAAiE5DdnQAzwjIIxyQRzggj3BAHuGAPMKBCqWWulKNUo6rZASBG7RqPZQ67QrDjUyhkNx4FDceLSCU0fkKSZ3pP/55U1ZSqCwtVIbHskkk4MaluvvSdWqi82HZGwaL3NKAqeQ4AKTiAkV4b3ZYDDuqL8/mCm30mHdFknWuubuQExbDDo9h27x7Z8BgAKWFypJCRXG+sv9YL+FAvg2VdNhjfbnm7Ld13eM4A172olBJNuzSacExw/VT4vIi1eg5/r7BHTvYO+bxbqasKEs6doHAjUvpeJyugVJKnD5QEzOAH92vA4d5Bzw+zFVUPVANnepra4SuxO9HGkKj2d2F1p6yrPV481yzXIIPn/5cSDSS8UMD34faJ9nTmsJW9R+L8xVNddrnSiIAYESKb0OltqRQaU1hyx4ljdjDHMWYuQEwYnMxxs4PuJ8tk4pxiyUte7z2i7hXEhdSYK5Hr0Te9VONFotZ8FhbptEoibDert1D7AzhsWyFFK+v0JovZsFjUZZs4ARvqIG5Hv8Y7130h9R8GXMetSp9Sb7CvxsTdmDmSEtL27hxow0bjhgxorq62g4RgYBw1oMcOaY1d9/AnMeSQkVYl//mu3v3rg1bVVVVSSQSO4TzmPAYjvkLt7n+46WjjWEx7G5RbvaIrKSkZM+ePdnZ2RQKRSgUzp49Oy4ubsGCBXl5ecYCR44c6dGjR1pa2tWrVwsLCxkMRlJS0ltvvSUQCAAAqampdDrdz8/v0KFDCxcu/Prrr41bDRs2bNu2bdCjLburKr+nHDzFp90Shvb5YVu5uEZrpoDNaLXa5OTkdevWPXz48N69eytWrBg2bJhGozEYDHPmzNmwYYOxWHZ2dmJi4r59+27dupWZmblgwYL58+cbV61evXrChAlvv/32lStXWlparl69mpiYWFVVZY9oDQZDQ5Xmxx0VZgqYu/+olBF2+h1dXl7e3Nw8Y8aMHj16AAC2bt2ak5OD4ziD8T93B0QiUVpaWmhoKIVCAQBoNJrU1FSFQsHhcCgUSmNjY1pa2hOb2Ak3LlUlM9eLbNejwQA0KoLFsYvHkJAQDw+PDRs2jB07NjExUSgUJiUlPV2MQqFUVlbu2LGjqKhIqXx8empubuZwOACAsLCwrpEIAGBzKSq5ufuq7V5nDHrAYNrrqQODwdi7d+/AgQMPHz48f/78SZMmnTt37uliFy5cSE1NjYuL279/f3Z29s6dO5+oxE7hmYAEaHQSaP9WRLumyBQASECjstdDgtDQ0OXLl6enp+/YsSM8PHzdunUPHjx4osyJEyfi4+MXLVpkPPwVCoWdgrGIWkFQ6WTQ/u1Wcy3O4knBZkpLS0+dOgUAYDKZQ4YM2bp1K5lMvnfv3hPFpFKpj8/fl8gLFy7YIxhrsHipMOdREM5SK+zysKWlpWXz5s07d+6sqqoqKSk5cOCAXq8XCoUAgODg4KKiouzs7JaWlp49e968efPOnTs4jn///ffGq01dXd3TFYaGhgIAMjIybOt+WkQtJwLCWGYKmPPoE0h/kCO3Q1QgISFh7dq1Z8+enThx4tSpU/Pz8/fs2WN0MXnyZIPBsGTJkuLi4qVLl/bt23f58uX9+/cXi8WbNm3q1avXkiVLnm6YQUFB48aN+/LLL3ft2mWPgB/myi08aTDTJ1LK8P0bSuzQG3M99q4rVitwMwXMnx8pQT3dxNUWbnU88zRU6kKj2Ey2ufOjhXEAkYncG+lN498UtFdg0aJFT18fAAA4jgMAqFTT9aenpxv7gNDJz89ftmyZyVU4jrcXDwDg4sWLJJLp6/GN9MakERaeLlh+PnNiV3XfUZ6BPUyfZRsbGzEMM7lKq9W218Uz/ka2EzU1NTZs1V5IlQ/Ut39vnrg40Pzmlj02VGjzr0tHzHi+Hs60knG4XjTY3TvIQp/f8i8W3xCGfzfGxaMN8GJzGS6kNQh6sCxKtPZ5YcwAPplMyjzdBCM2l+H6KTGNQbZyNEAHxgHkXZGoFfoXXrLqea6rcyO9ietOjbV6rE8H7kTEDXInU8HpA7W2xuYaGAwgfV8NnUm2XqIt46RKCpXnvq3tN8YrcbhHx4N0drJ/a8nOaB79mn9oBx+R2jhuL/N0U1GWLLofL6w32z+0Sx+E2YPaMk1pofJupjT2Rf4LL3nZUIPt40h1an3BdWnpXaWkURceyyVTAJtH4XvRcMwFXmyi0klSMaaUEXrCUFyg8PClh/VmCwe60xg2jkTs1HhcIxqlvrZUo5BiKhlhMACVHPKttvPnz48aNQpunW48CgmQ3HgUjjstIIzJdOvsHWsIHu1Nnz59bt265egoLIDeV4AD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuHgAh75fFsmeOpiXMCjVGrhXXxnwAU8ugTIIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEg/O+hxQfH08ikUikxxEaJ4+4ffu2o+MyjfO2R4FAQCaTSSQSmUw2fggIcN45o53XY3x8fNtjhSAI44RTzonzekxJSfH392/9GhgYOGvWLIdGZA7n9RgdHR0fH9/6VSQSRUdHOzQiczivRwDA9OnTjU3S399/5syZjg7HHE7tMSYmxnhOTEhIiIqKcnQ45oCTn8uIQQ9qStWSBkyjgjbb4cCY12QV3v2jxt7+vQVWnUw3iocvLSCMRYLXiqD1H2tLNdd+EZMAKaC7G252ynKHQ6WTa0qUAIB/TPSGNcs8HI8NldrLxxtHzAyk0lwm0xSuM2T8UD14io+vFdNFWQRCy9aq9Ce/rB49N8iFJBqn+hg9N+jEF1XmJ/y3EggeszNaEoa7ai6LhOHe2RkQzrwQPNaVq919aJ2vxyHwfeh1ZZrO1wPjuFbqWTyY1/2uhM2jqpUQehcQPBJ6g5kJyp0cgwHoCQjRO3U/3IVAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCwfEeX502Zt9+08l3xk0YcviHb8xvfuz4keHJfe0TWgdwjMdNm1edOfuzxWLTp82JjRF1RUCdxjEe7923KovWzJR5QmG8FQUdT1d71Ov1Q4cn1dfXbd+xZcKk4caFVCrt+PEjyaNeeHn84DXvL5fJZcblrcf1sWM/Tnl1VHl56Zx5rwwdnrTgjennz6c/XTlBEKkrl8x6bZJW29U5nLraI5lMPnfmOgBgZer6n0/8blx48dKvao1629YvUlesz8u7/e3BPU9sRaPT5XLZ5//Zuvq9TRcybg18ccj2T7eIxU+mSd+244NHxQ+2bf2iS1NEAgD5+bXNcDjcmSnzjJ+vXbtYkJ/zRAEymYxh2Ly5i6KiYgAAI0e+/N2hfY8e3ff2/ju74cHv9l68+OvnO/cJAizkLrIHjr9eAwDaXkx4fHetzvRRGRnZ2/iBy+UBABRKhXFcJIlEyvj93LcH96xdsyXqrzJdjFN4bJt+rL1kY+2tMhgMBEF8snWjsV3bLUYLOIXHzrPi3fdHjhz78ScbJBJow1c6xLPgkUwmjxk9fvmy1UwGc+v2zY6Joet3yWAwfHx879y5mZObbUxzCAUWi7V2zZasrOvHT6TBqtN6HNMeZ6bMz76dtX7DCp1OB7Ha3r2Fr81+fc/Xn7e0NEOs1hogjJM69K/yYTMEPE+XHFIhFWOXfqqZtaZbJ+t5Fs6PzgDyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4QPDI9aLhWld9YQHT6fleEO5UQfDI96A2Vqs7X49DEFdpeE7iMWaAe0mBvPP1OISSAnnMAAjzakPw6BNEFw7kX/lvXeer6mIuH60TDXb3CqB3vipo718X3pAVFyjZfKpvCAvKG1L2g0wmNVSoFRK8ZwI7uh8PSp0w50GSNGAV91XyFlwpg5naPjc3TySKg1ghm0flelK7RbrxvaE9C3He+aRaQXntnyOQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAcX8Ojt7QKTaruAR7FY7OgQLOMCHl0C5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wsF530MSiUQUCsU446hxMlK9Xp+T8+TUuU6C87ZHgUBgnPu2Na99UFCQo4NqF+f1KBKJ9Pq/M4YSBBEbG+vQiMzhvB6nT58uEAhavwYFBaWkpDg0InM4r0ehUNi2AQqFwpiYGEcGZBbn9QgASElJ8fX1Nea1nzFjhqPDMYdTe4yNjTWms4+Pj3fmxmhVXoCWBkxcrVXKYb6abj3D+yxQ1Hi/GDsp94rEIQFweFRvAcPd18Ib72b7jwaQfqBW3ozzfegMFgV+jK6ARknIm3U8L+pL8wLMFGvXo14Pjn9RHdXPPSSSbbcgXYbyIsX9bOnkpYHtZS1o1+PJr2oi+7gH9nCzb4CuQ9UD1cMcyfiFApNrTV9naks1JBIJSWxLUE83gx7Ul5tO3m7ao7hG68Z1itQ0TgWLQxXXmp6A37RHtZxg85HHJ2HzqSqp6X6LaY+wsr0/Y+j1oD0pTt0PdyGQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7h8Ix73LR51ZmzP3fBjp5xj/fu3+2aHZl+rpB1thnDQNxgT+sramoSb9226W5RfkhI2KQJU0vLim/eurF/7xEAgFjcuPvLz+4W5Wu12r59B8x5bWGgIAgA8OjRgzfeTNm96+DhHw5cv37Z19dv6JCRby5cZszPXFCQe/C7r+/fL/L08n6h38C5c95ksVgAgP8e++FI2nfL31m9afOqyZOmL1n8z8zMqxcuns/Lv6NQyKMiY2bPel0kSsRxPHnUC8bYeDy+Mff7mbM/n0o/XlZWHB4eMWzoqCmTp3dIVu6lZgYT9B1lQgu09rht++bKyvJPd3z1wabt165fun07y6gDx/F3UxcVFOamrlj/zf6fuFze4sWza+tqAAB0Oh0AsOPTLckjXvr1XObqVZvTfjp06XIGAKCiouy91UsxHNu96+DG9Z88fHjv3dRFxuE+NBpdrVYdSftu7Zot48e/olKpPvzX+ziOr1n9wUcf/jswMPj99f+USFqoVOq5M9cBACtT1xsl/vbbme07tkT2iv7x8Kl5cxf9dPTQ7i//DevPh+OxqUl881bm9OlzIntF+/j4rnj3/ZraKuOqvPw7lZXla1Z/0CfpBQ8Pz7cWv8vhcI8d+9GYbxkAMGRw8uBBw2k0Wrwoyc/P/8GDPwEAGb+fpVFpH2zaHhzcLTy8x4oV6+7du3sj8woAgEKhqFSqBfOXDBs6Migw2M3Nbd/eI8vfWR0vSooXJS18Y5lKpSoszHs6yFOnjwuF8e8sW+Xu7pGU2G/OawuPnzgik8ugGIDjsbSsuG16ej7fXSRKMn4uKMil0WgJ8X0e749MFsYlFBT8PYyxZ8+o1s8cDlehkAMACgvzIiN78/nuxuWBgiB/v4C8vDutJXv1jG79rFIq//N/216ZOnro8KRxE4YAACTSJ7OJ4zheVFTQJ6l/65L4+D4EQRj/bZ0HzkMYpVIBAGCyWK1LeFx+XV0NAEChkGMYNnR4UtvyXl5/v+JvbJVPoFDIHz66/8RWLS1NrZ+N5wQAQF1d7Tv/fL1PUv8N6z6Ojo4lCGL0Sy8+XaFGoyEIYv+B3fsP7G67XCqFM0wDjkcGnQEAINokBW+RPM5A7eXlzWKxPvrwf85EVIqF/Xp6eceyWPPmLmq7kM9zf7rkhYvnMQxb9d4mJpNpxguHw2EymaNHjRs0aHjb5SHBoVb8fZaB41EgCDIe3cHB3QAAMrksNzc7MDAYABAeHqFWq/39BQH+j5+gV9dUeXp4ma+we3jExYu/iuISSX8NYCgrKwkKCnm6pFQq4XJ5RokAAONlyiTh4RFqjTr+rxOOTqerr69te2R0Bjjnx5CQ0ODgbt8e3FNTWy1XyHfu/NhoFgDQr++Avn0HbN/+QX19nUTScvxE2qJFs87/mm6+wqlTZ+ME/sXuTzUaTUVF2Vd7Pp//+rTy8tKnS/bo3rOpSXz6zEkcx//Iul5YmMthcxoa6gAADAbDx8f3zp2bObnZOI6/+cayK1d+P3P2Z4Ig8vNzNm9ZvWLlYgzDoBiA1u9ZtXKjXq+fNXtiauri3tHCqMgYGvXxGK2PP9o5aNDwDz5cM2lK8s+/HB0zZsLECa+ar43P4+/fl8ZkMF9fOGPOvFfy8u+sWrmxe/eIp0uOGDFmZsq8b779KnnUCydOpr29dGXyyLGHvt//f7t2AABmpszPvp21fsMKnU4nFMbv+fL7/PycSZNHvLd6qVql+nDLZzQanNQp0PrhUqlEo9H4+fkbv763aimbzdm44RMoUToJXdEPX78x9d0Vb167dqmlpfngd3tzcrNffnkyrMqdH2jtUSJp2f7plvLy0qamxm4hYXNeW9i//z+ghup4zLRHaIN43N09PtryGazaXI5n/H5Pl4E8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMLBtEcm+zl9m9ACBsBqx4xpj57+9IYKV01Vbz/qK9Se/qaTjpv2GBzB0qj1KqhprF0dpRTHdPrA7iyTa9s5P5LAmDn+V0/U6zR60wWeM7Qq/bWT9S/N9Qcdfd8VACBpxH76d2X3OB7fm85we06vSFoFIW3WlRTIpy4PNpO/3fI8SEV/yBurtXBT1XeIoqKi6OhoKwraBTaP4hPEiO7HM1/MeeeTagXltX+OQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMcXMCjv7+/o0OwjAt4rKurc3QIlnEBjy4B8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4eC87yElJCQY09kbp4A0GAwGg+HOnTtWbOoAnLc9BgQEGNPZG7+SSKTAwEBHB9UuzutRKBS2PVb0er0D3zK0iPN6nDZtWtu89oGBgSivvS2IRKLIyMjWr0KhMC4uzqERmcN5PQIAZs6c6eXlBQDw8fGZNm2ao8Mxh1N7FIlExnT2MTExQqHQ0eGYA2YyXJWMUMlxpYzQqvQ6LQGlzuR+82VV/OF9phTekEKpkM4gM9wobB6FzaeyONCmhYHQf2yo0BYXKB/lKcg0qlaJUxkUOpuux5y0W0qmkXRKHa4jGG5UPY5HxHHCYth+IYxOVtspj/Xlmisnmgg9icJkcL3dmFzTc7I4LRq5Ti5W6bU6CkU/aKK3byds2u7xt8MNteVar1BPtgfT5t07CYpmTVNZsyCckTzD17YabPGokODff1IR1NuX4216MhsXRSFWVxc1zFrdjc3v8Hmzwx6lzfhPn1WG9wuiUJ36Wm8bBKYvzqqanhrM8+jYFbhjHsU12lP7GsL6CKwo68KU3qoev9Dfq50puEzSgTZlMIAjOyqfeYkAgLA+gT9uq+jQJh1oj8e+qOX4ezLYMLucTotWiSnrWya/FWBleWvbY+5liQ6jPCcSAQAMNk2jJeddtbbzb63HzNNNfhEdSLfwDOAX4Zl5usmKgsBajzmXJP4RnmRKO3PNPaNQqGT/7u55l61qklZ5LMyUsdydt7N99OePP901yx41M/iswj8geZQ141q1nslxsd98UGBx6So5oZBYnmvQssfyP5Xu/hxIgbkeHgJu2Z9Ki8UsX38bKrVkmh0bY9btX7KyT9bVFwf4R4hik//R//H92vUfjRiTvFgub/rt0n4mg90rov+El97lcb0AAFqt6vB/NzwqyQ7w6/Fiv1fsFxsAgESlNFbqQH8LxSy3R4WUoDLsNX3z7dyzR09+FCSIWrvi5KhhCy9fP/zL2c+Nq2g0xoUr39FojC1rM1YuSyspy/nt0n7jqp9OfiRuqlw8f/ecGVurax88ePSHncIDANAYVDmU41opxWl28/hH9snwbvGTx63ksD169uibPPT1a3+kKZXGXI4kX++QYYPmsFhcPs+nZ/e+1TX3AQBSWWNeYcbQgbODA6N5XK+XR71NpdjxcKEyKNbMxWrZI5VOIVPs4pEg8PLKgp4R/VqXRIQn6fVEafnjLLdBgX+nfmWxeGqNHADQ3FINAPDzDTMuJ5FIQYLIp+qGBplCptIs//mWz48UigHTYPb4JaPDNHo9cS7jq3MZX7VdLlc2//XRRI9VqZICAJiMvy99dLodb99hGpxqRYpDy3bYfKoG0sOWJ2AxOXQaMyn+ZWHvYW2Xe3sFmYvHjQ8AwHBt6xKN1vL11GZwLc7mW7ZkuYR3IKOi2F6ziAf4R+gwdY/wRONXDNe1tNS68/3MbOLhLgAAlFcWBAb0BADodJpHJdk8no+dItQTBm+B5fOv5fNjYHemrEEBKaonGTvyrfy7F7Ju/0IQRElZzqG0tXu+XYrhOjObuPN9Q0PizmV8JW6qxDDt4aPrSaYyP8NC1qBobw77tlhujwGhTK0SIzA9hQY/3PDQ+OWLDl64cjD93H9wQhcSFDNv5nYa1cL/f8aUjcdObf1s1yycwPomjE8Sjb3/MBN6bAAAXEdgGtyap4lW3X+8fLxJKqPx/NiQwnMZJLVKTw9s0CQLWaatvU8RP4TfUNxsRcFnjcaSpoShfGtKWtWb4XlSQ6PdmqvknkFckwVu3Dx25rfdJlcRBEahmO44pEzZHB050JoArOHSte8zLn9jchWLyVNrZCZXzZ/1aXg3kclVTZWy7rEcjrtViqx9rqBV6Y/trhX0Nj3FAYbrcExrcpUO09Bppu+50eksiqUE99aDYVq8nQsUjmPUdjqBZmKoKax75e0AOtOqQ7YDz2dK7yqvnZIEx7nAbBGdpyK3dvAkz26RblaW78AlOKw3u1eCW919sa2xuQy198TRfdjWS7RlHEBhpjw/UyWI8u54eK5BzZ/iuBfZvft17JZrh7uEMf25veLolXkuMIeJDVTm1UbGMzoq0fZxUhX31ZeOiTnebM9gq7oFzk9ThVTZpBj2qk9QhC13PWwfb6bHwfV0cVGWzDvUg+PFYrCtuCvifGgVmKJF3VjSEtOfP2Ccl82/MDs7jlSjJHIuSR/ckWOYge/HNQBAY1BoTBoATjqOFJAApsYxLQEAkNXJaQxSr0Ru/GD3TiYgg/Y+l1SM1ZRomut1Cilh0AOFBINSLXQ47jQSGXD4FE8/uiCcaSZ1WYdw3vfiXItncAyjQ0Ae4YA8wgF5hAPyCAfkEQ7IIxz+HxDUFTTxwYFRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a58b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit after the decimal point:\n",
      "\n",
      "- The tenths place: Both numbers have 9.\n",
      "- The hundredths place: 9 has 0, while 1.11 has 1.\n",
      "\n",
      "Since 1 is greater than 0 in the hundredths place, 9.11 is larger than 9.90.\n",
      "\n",
      "Therefore, 9.11 is greater than 9.9.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you look at each digit starting from the left. So, 9.9 is the same as 9.90 when considering the hundredths place. Now, comparing 9.90 and 9.11. The tenths place is 9 in both, but the hundredths place is 0 vs. 1. Since 1 is bigger than 0, 9.11 is larger. So the answer should be 9.11.\n",
      "\n",
      "Wait, but the user provided an inference that I need to use. Let me check the original response. The assistant said that 9.9 is 9.90 and compared the hundredths place. Yeah, that's right. So the key point is that even though the tenths are the same, the hundredths differ. So 9.11 is bigger because 1 is greater than 0. Got it. Need to present this in a conversational way without mentioning the inference process, just the conclusion.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.11**입니다.  \n",
      "\n",
      "두 숫자는 전체 숫자(9)가 같지만, 소수점 아래의 숫자를 비교해야 합니다.  \n",
      "9.9는 9.90과 같으며, 9.11은 9.11로 비교할 때, **하나의 자리에서 1이 더 큰 숫자**가 있습니다.  \n",
      "즉, 9.11은 9.9보다 더 큰 숫자입니다.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you look at each digit starting from the left. So, 9.9 is the same as 9.90 when considering the hundredths place. Now, comparing 9.90 and 9.11. The tenths place is 9 in both, but the hundredths place is 0 vs. 1. Since 1 is bigger than 0, 9.11 is larger. So the answer should be 9.11.\n",
      "\n",
      "Wait, but the user provided an inference that I need to use. Let me check the original response. The assistant said that 9.9 is 9.90 and compared the hundredths place. Yeah, that's right. So the key point is that even though the tenths are the same, the hundredths differ. So 9.11 is bigger because 1 is greater than 0. Got it. Need to present this in a conversational way without mentioning the inference process, just the conclusion.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.11**입니다.  \n",
      "\n",
      "두 숫자는 전체 숫자(9)가 같지만, 소수점 아래의 숫자를 비교해야 합니다.  \n",
      "9.9는 9.90과 같으며, 9.11은 9.11로 비교할 때, **하나의 자리에서 1이 더 큰 숫자**가 있습니다.  \n",
      "즉, 9.11은 9.9보다 더 큰 숫자입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cec5a4",
   "metadata": {},
   "source": [
    "### 2개의 모델을 연동한 코드를 Gradio 를 사용하여 UI로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f03026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pshcc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-2-xujkvS_f-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\pshcc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-2-xujkvS_f-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: asd\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 9\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate 함수 - 질문: asd\n",
      "[DEBUG] generate 함수 - 추론 길이: 9\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 1356\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's tackle this query. The user provided a question \"asd\" and an inference process that's just a blank. My job is to create a Korean answer based on that. First, I need to understand what \"asd\" could mean. It's probably a typo or an abbreviation. Maybe it's \"asdasd\" or something else. But since the inference is empty, I need to figure out the best way to respond.\n",
      "\n",
      "The user's instructions say to analyze the given inference and create a structured answer in Korean. But since the inference is empty, I should focus on the question itself. \"asd\" is unclear, but maybe it's a test or a placeholder. I should consider common scenarios where \"asd\" might be used. Could it be a code, a term, or a random string?\n",
      "\n",
      "Since the user wants a response in Korean, I need to make sure the answer is natural and relevant. The key here is to address the question directly without mentioning the inference process. So, I'll explain that \"asd\" is unclear and suggest possible interpretations. Maybe it's a typo, a code, or a term. I'll list some possibilities and offer assistance to clarify. That way, the answer is helpful and meets the user's requirements without violating the guidelines.\n",
      "</think>\n",
      "\n",
      "\"asd\"는 명확한 의미가 없어 추가 정보가 필요합니다. 하지만 일반적인 해석으로는 키보드 누르기(ASD 키 조합), 애니메이션 또는 코드 등의 가능성이 있습니다. 만약 특정 맥락에서 해당 단어를 사용한 경우, 추가 설명을 요청해 주시면 더 정확한 답변을 드릴 수 있습니다.\n",
      "[DEBUG] 입력 질문: 파이썬에 대해 알려줘\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 4535\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "Okay, so I'm trying to learn Python. I've heard a lot about it being useful for programming and data analysis, but I'm not entirely sure where to start. Let me think through how I can approach...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬에 대해 알려줘\n",
      "[DEBUG] generate 함수 - 추론 길이: 4535\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "Okay, so I'm trying to learn Python. I've heard a lot about it being useful for programming and data analysis, but I'm not entirely sure where to start. Let me think through how I can approach...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 673\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "</think>\n",
      "\n",
      "파이썬은 간결하고 명확하며 다양한 데이터 타입을 지원하는 인터프리터 언어로, 프로그래밍, 데이터 분석, 웹 개발 등 다양한 분야에서 널리 사용됩니다. 기본적인 시작은 변수 선언과 데이터 타입부터 시작하되, 함수, 조건문, 루프, 예외 처리, 데이터 구조 등은 핵심 기능입니다. 변수는 값 저장을 위해 사용하며, 타입(정수, 문자열, 부울값 등)을 명확히 지정할 수 있어 코드의 가독성이 높습니다. 함수는 코드의 재사용성을 높이는데 도움이 되며, 파이썬에서는 모듈을 통해 외부 라이브러리(예: `math`, `random`)를 활용할 수 있습니다. 조건문과 반복문은 프로그램의 로직을 구현하는 데 필수적이며, `if-elif-else`와 `for`, `while`을 통해 다양한 로직을 작성할 수 있습니다. 예외 처리(`try-except`)는 코드의 안정성을 높이는데 도움이 됩니다. 데이터 구조(리스트, 튜플, 딕셔너리, 세트)는 데이터를 효율적으로 관리할 수 있게 도와줘요. 함수의 매개변수와 반환값은 코드의 재사용성을 높이는데 중요한 역할을 합니다. 프로그래밍을 시작하는 것은 단순한 과정이지만, 기본적인 개념을 익히고 실무에서의 활용을 늘려가는 것이 중요합니다. 파이썬은 학습이 쉬우며, 다양한 분야에서 활용 가능하다는 점도 학습 motivation이 됩니다. 🌟\n",
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 468\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 468\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 671\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. The explanation given is that both start with 9, so they align the decimals to 9.90 and 9.11. Then compare each digit: tenths are both 9, but hundredths are 0 vs 1. Since 1 is bigger, 9.11 is larger. So the answer is 9.11.\n",
      "\n",
      "I need to present this in Korean, keeping it natural. Make sure to mention the alignment of decimals and the comparison of the hundredths place. Avoid the process steps but just the conclusion. Check for clarity and correctness.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 9.11입니다. 두 수는 정수부가 같으므로 소수부를 비교해야 합니다. 9.9는 9.90으로 바꿔두면, 9.11이 9.90보다 소수점 아래의 1이 더 큰 수로, 결국 9.11이 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    #model=\"qwen2.5:1.5b\", \n",
    "    model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    #launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394d68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-2-xujkvS_f-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
