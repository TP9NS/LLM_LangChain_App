{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, the user asked about LangChain. I remember they're related to natural \"\n",
      " 'language processing.\\n'\n",
      " '\\n'\n",
      " \"LangChain combines PyTorch with LLMs and NLP models like Sentence-BERT. It's \"\n",
      " 'used for tasks like chatbots and text generation.\\n'\n",
      " '\\n'\n",
      " 'I should explain its core componentsâ€”PyTorch, LLMs, and how it works '\n",
      " 'internally.\\n'\n",
      " '\\n'\n",
      " 'Maybe mention the pipeline: data transformation through transformers and '\n",
      " 'then sentinels. Also, list use cases briefly.\\n'\n",
      " '\\n'\n",
      " \"Make sure to keep it clear and concise since they're new to this topic.\\n\"\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is an open-source Python library that integrates PyTorch deep '\n",
      " 'learning with large language models (LLMs) to perform tasks related to '\n",
      " 'natural language processing. It provides a high-level interface for building '\n",
      " 'chatbots, text generation systems, and other NLP applications.\\n'\n",
      " '\\n'\n",
      " '### Core Components of LangChain:\\n'\n",
      " '1. **PyTorch Integration**: LangChain leverages the power of PyTorch for '\n",
      " 'efficient computation and flexibility.\\n'\n",
      " '2. **LLMs (Large Language Models)**: It uses LLMs, such as Sentence-BERT or '\n",
      " 'BERT, to generate text responses or perform other NLP tasks.\\n'\n",
      " '3. **Pipeline Framework**: The library provides a pipeline for building '\n",
      " 'systems that combine text generation with the generation of user questions '\n",
      " 'or prompts.\\n'\n",
      " '\\n'\n",
      " '### How LangChain Works:\\n'\n",
      " 'LangChain works by defining a series of components within a PyTorch-based '\n",
      " 'framework. These components include:\\n'\n",
      " '- **Preprocessing and Text Generation**: Generating responses to user '\n",
      " 'queries.\\n'\n",
      " '- **Text Generation**: Using LLMs to produce natural language outputs, such '\n",
      " 'as chat completions or text generation for applications like chatbots or '\n",
      " 'content creation.\\n'\n",
      " '- **Question and Prompt Generation**: Creating questions that guide the '\n",
      " 'system towards specific outcomes.\\n'\n",
      " '\\n'\n",
      " '### Use Cases:\\n'\n",
      " 'LangChain is used in various NLP tasks, including:\\n'\n",
      " '- **Chatbots**: Building systems that can interact with users by answering '\n",
      " 'questions.\\n'\n",
      " '- **Text Generation**: Generating text from prompts, such as writing '\n",
      " 'proposals or writing descriptions of products.\\n'\n",
      " '- **Content Creation**: Automating the creation of content based on user '\n",
      " 'input.\\n'\n",
      " '\\n'\n",
      " '### Example Workflow:\\n'\n",
      " '1. Define a prompt to get information about a topic.\\n'\n",
      " '2. Use LangChain to process this prompt and generate an answer.\\n'\n",
      " \"3. Use the system's response to form a new question that asks for additional \"\n",
      " 'details.\\n'\n",
      " '\\n'\n",
      " '### Summary:\\n'\n",
      " 'LangChain is a powerful tool for building systems that require natural '\n",
      " 'language processing, particularly those involving AI-driven responses or '\n",
      " 'automated content creation. It combines PyTorch with LLMs to provide '\n",
      " 'flexible and efficient solutions for various NLP tasks.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it's open-source and has a large community. Also, it's used in various fields like web development, data analysis, artificial intelligence, and more.\n",
      "\n",
      "Wait, the user might be a beginner, so I should keep the explanation simple. Maybe start with the basic definition, then touch on its features and applications. Oh, and include that it's written in Python syntax, which is easy to read. Also, mention that it's cross-platform, so it works on different operating systems. \n",
      "\n",
      "I should make sure not to use any technical jargon that's too complex. Maybe list some key points like being free, having a vast ecosystem, and being used in both small and large projects. Also, note that it's interpreted, which means it's not compiled like C++. \n",
      "\n",
      "Oh, and maybe add a sentence about how it's popular because of its simplicity and the extensive libraries available. That would make the answer more comprehensive. Let me check if there's any other important aspect I should include. Maybe the fact that it's used in data science with libraries like Pandas and NumPy. \n",
      "\n",
      "Wait, the user didn't specify the level of detail, so keeping it concise but informative is best. Alright, structure the answer with a clear definition, key features, and applications. Make sure it's in Korean and flows naturally.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ **í”„ë¡œê·¸ë˜ë° ì–¸ì–´**ì…ë‹ˆë‹¤.  \n",
      "ê·¸ê²ƒì€ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ë¬¸ë²•ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ë©°, ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ì¸ê³µì§€ëŠ¥, ì•± ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ë„“ê²Œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.  \n",
      "íŒŒì´ì¬ì€ ì˜¤í”ˆì†ŒìŠ¤ì´ë©°, ì»¤ë®¤ë‹ˆí‹°ê°€ í™œë°œí•˜ì—¬ ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë„êµ¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.  \n",
      "ë˜, ì½”ë”©ì´ ì‰¬ì›Œì„œ ì´ˆë³´ìë„ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆì–´ìš”.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ qwen2.5:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? í•œê¸€ë¡œ ë‹µë³€í•´ ì¤˜\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
      "\n",
      "To make a fair comparison, it's helpful to have the same number of decimal places for both numbers. \n",
      "\n",
      "I can rewrite 9.9 as 9.90 by adding a zero in the hundredths place. This way, both numbers will have two decimal places: 9.90 and 9.11.\n",
      "\n",
      "Now, I'll compare each corresponding digit from left to right.\n",
      "\n",
      "Starting with the units place, both numbers have a 9, so they are equal here.\n",
      "\n",
      "Next, in the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Align the Decimal Places\n",
      "First, it's helpful to write both numbers with the same number of decimal places for a clear comparison.\n",
      "\n",
      "- **9.9** can be written as **9.90**\n",
      "- **9.11** remains **9.11**\n",
      "\n",
      "### Step 2: Compare Digit by Digit\n",
      "Now, compare each corresponding digit from left to right:\n",
      "\n",
      "1. **Units Place:**  \n",
      "   Both numbers have a **9** in the units place.\n",
      "\n",
      "2. **Tenths Place:**  \n",
      "   - **9.90** has a **9** in the tenths place.\n",
      "   - **9.11** has a **1** in the tenths place.\n",
      "\n",
      "Since **9 > 1**, **9.90** is larger than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "Therefore, **9.9** is bigger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
       "\n",
       "To make a fair comparison, it's helpful to have the same number of decimal places for both numbers. \n",
       "\n",
       "I can rewrite 9.9 as 9.90 by adding a zero in the hundredths place. This way, both numbers will have two decimal places: 9.90 and 9.11.\n",
       "\n",
       "Now, I'll compare each corresponding digit from left to right.\n",
       "\n",
       "Starting with the units place, both numbers have a 9, so they are equal here.\n",
       "\n",
       "Next, in the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Align the Decimal Places\n",
       "First, it's helpful to write both numbers with the same number of decimal places for a clear comparison.\n",
       "\n",
       "- **9.9** can be written as **9.90**\n",
       "- **9.11** remains **9.11**\n",
       "\n",
       "### Step 2: Compare Digit by Digit\n",
       "Now, compare each corresponding digit from left to right:\n",
       "\n",
       "1. **Units Place:**  \n",
       "   Both numbers have a **9** in the units place.\n",
       "\n",
       "2. **Tenths Place:**  \n",
       "   - **9.90** has a **9** in the tenths place.\n",
       "   - **9.11** has a **1** in the tenths place.\n",
       "\n",
       "Since **9 > 1**, **9.90** is larger than **9.11**.\n",
       "\n",
       "### Conclusion\n",
       "Therefore, **9.9** is bigger than **9.11**.\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is definitely bigger than 0.11. Therefore, even though both numbers have the same whole number part, the decimal part of 9.9 is larger, making 9.9 bigger than 9.11.\n",
      "\n",
      "Let me double-check. If I write them out: 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So the answer should be 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "**ì´ìœ :**  \n",
      "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë©°, ì†Œìˆ˜ì  ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "- 9.9ëŠ” 0.9ë¥¼, 9.11ëŠ” 0.11ì„ ì†Œìˆ˜ì  ë¶€ë¶„ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤.  \n",
      "- 0.9ëŠ” 0.11ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9 > 9.11ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ë‹µë³€:** 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3623342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
       "\n",
       "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
       "\n",
       "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is definitely bigger than 0.11. Therefore, even though both numbers have the same whole number part, the decimal part of 9.9 is larger, making 9.9 bigger than 9.11.\n",
       "\n",
       "Let me double-check. If I write them out: 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So the answer should be 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
       "**ì´ìœ :**  \n",
       "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë©°, ì†Œìˆ˜ì  ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
       "- 9.9ëŠ” 0.9ë¥¼, 9.11ëŠ” 0.11ì„ ì†Œìˆ˜ì  ë¶€ë¶„ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤.  \n",
       "- 0.9ëŠ” 0.11ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9 > 9.11ì…ë‹ˆë‹¤.  \n",
       "\n",
       "**ë‹µë³€:** 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek ëª¨ë¸(ì¶”ë¡ )ê³¼ Qwen ëª¨ë¸(í•œê¸€ì‘ë‹µ)ì„ ì—°ë™í•˜ê¸°\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f146748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\n        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\\n        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\\n        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\\n        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n        ì§€ì¹¨:\\n        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\\n        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\\n\\n        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        ì§ˆë¬¸: {question}\\n        ì¶”ë¡ : {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.11**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.11ì€ 9.11ë¡œ ë³´ì…ë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì•„ë˜ ì²« ë²ˆì§¸ ìë¦¬(ì‹­ë¶„ì˜ 1)ì—ì„œëŠ” ë‘˜ ë‹¤ 9ì´ë¯€ë¡œ, ë‘ ë²ˆì§¸ ìë¦¬(ë°±ë¶„ì˜ 1)ì—ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 0ì„, 9.11ì€ 1ì„ ê°€ì§‘ë‹ˆë‹¤. 1ì€ 0ë³´ë‹¤ í° ìˆ˜ì´ë¯€ë¡œ, 9.11ì´ ë” ì»¤ì§‘ë‹ˆë‹¤.  \n",
      "\n",
      "ë”°ë¼ì„œ, **9.11ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤**.\n",
      "{'question': '9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit after the decimal point:\\n\\n- The tenths place: Both numbers have 9.\\n- The hundredths place: 9 has 0, while 1.11 has 1.\\n\\nSince 1 is greater than 0 in the hundredths place, 9.11 is larger than 9.90.\\n\\nTherefore, 9.11 is greater than 9.9.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\\n\\nI remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\\n</think>\\n\\n9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.11**ì…ë‹ˆë‹¤.  \\në‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.11ì€ 9.11ë¡œ ë³´ì…ë‹ˆë‹¤.  \\nì†Œìˆ˜ì  ì•„ë˜ ì²« ë²ˆì§¸ ìë¦¬(ì‹­ë¶„ì˜ 1)ì—ì„œëŠ” ë‘˜ ë‹¤ 9ì´ë¯€ë¡œ, ë‘ ë²ˆì§¸ ìë¦¬(ë°±ë¶„ì˜ 1)ì—ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 0ì„, 9.11ì€ 1ì„ ê°€ì§‘ë‹ˆë‹¤. 1ì€ 0ë³´ë‹¤ í° ìˆ˜ì´ë¯€ë¡œ, 9.11ì´ ë” ì»¤ì§‘ë‹ˆë‹¤.  \\n\\në”°ë¼ì„œ, **9.11ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤**.\"}\n",
      "==> ìƒì„±ëœ ë‹µë³€: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit from the left. So, the first digit after the decimal is the tenths place. Both have 9 there. Then the next digit is the hundredths place. The first number, 9.9, has 0 in the hundredths place because it's just 9.90. The second number, 9.11, has 1 in the hundredths place. Since 1 is greater than 0, that makes 9.11 larger. Wait, but the user wrote 9.11, which is 9.11, so the hundredths place is 1. So yeah, 9.11 is bigger. I should make sure I didn't mix up the digits. Yep, 9.9 is 9.90, so 9.11 is 9.11, which is higher. Got it.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.11**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.11ì€ 9.11ë¡œ ë³´ì…ë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì•„ë˜ ì²« ë²ˆì§¸ ìë¦¬(ì‹­ë¶„ì˜ 1)ì—ì„œëŠ” ë‘˜ ë‹¤ 9ì´ë¯€ë¡œ, ë‘ ë²ˆì§¸ ìë¦¬(ë°±ë¶„ì˜ 1)ì—ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 0ì„, 9.11ì€ 1ì„ ê°€ì§‘ë‹ˆë‹¤. 1ì€ 0ë³´ë‹¤ í° ìˆ˜ì´ë¯€ë¡œ, 9.11ì´ ë” ì»¤ì§‘ë‹ˆë‹¤.  \n",
      "\n",
      "ë”°ë¼ì„œ, **9.11ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë¸\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#ì‘ë‹µ ëª¨ë¸ (í•œê¸€ì²˜ë¦¬ ê°€ëŠ¥)\n",
    "#generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "#qwen3:1.7b\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwenë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "# invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ í˜¸ì¶œ\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"==> ìƒì„±ëœ ë‹µë³€: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6834b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a58b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit after the decimal point:\n",
      "\n",
      "- The tenths place: Both numbers have 9.\n",
      "- The hundredths place: 9 has 0, while 1.11 has 1.\n",
      "\n",
      "Since 1 is greater than 0 in the hundredths place, 9.11 is larger than 9.90.\n",
      "\n",
      "Therefore, 9.11 is greater than 9.9.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you look at each digit starting from the left. So, 9.9 is the same as 9.90 when considering the hundredths place. Now, comparing 9.90 and 9.11. The tenths place is 9 in both, but the hundredths place is 0 vs. 1. Since 1 is bigger than 0, 9.11 is larger. So the answer should be 9.11.\n",
      "\n",
      "Wait, but the user provided an inference that I need to use. Let me check the original response. The assistant said that 9.9 is 9.90 and compared the hundredths place. Yeah, that's right. So the key point is that even though the tenths are the same, the hundredths differ. So 9.11 is bigger because 1 is greater than 0. Got it. Need to present this in a conversational way without mentioning the inference process, just the conclusion.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.11**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "ë‘ ìˆ«ìëŠ” ì „ì²´ ìˆ«ì(9)ê°€ ê°™ì§€ë§Œ, ì†Œìˆ˜ì  ì•„ë˜ì˜ ìˆ«ìë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ì€ 9.11ë¡œ ë¹„êµí•  ë•Œ, **í•˜ë‚˜ì˜ ìë¦¬ì—ì„œ 1ì´ ë” í° ìˆ«ì**ê°€ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì¦‰, 9.11ì€ 9.9ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here.\n",
      "\n",
      "I remember that when comparing decimals, you look at each digit starting from the left. So, 9.9 is the same as 9.90 when considering the hundredths place. Now, comparing 9.90 and 9.11. The tenths place is 9 in both, but the hundredths place is 0 vs. 1. Since 1 is bigger than 0, 9.11 is larger. So the answer should be 9.11.\n",
      "\n",
      "Wait, but the user provided an inference that I need to use. Let me check the original response. The assistant said that 9.9 is 9.90 and compared the hundredths place. Yeah, that's right. So the key point is that even though the tenths are the same, the hundredths differ. So 9.11 is bigger because 1 is greater than 0. Got it. Need to present this in a conversational way without mentioning the inference process, just the conclusion.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.11**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "ë‘ ìˆ«ìëŠ” ì „ì²´ ìˆ«ì(9)ê°€ ê°™ì§€ë§Œ, ì†Œìˆ˜ì  ì•„ë˜ì˜ ìˆ«ìë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ì€ 9.11ë¡œ ë¹„êµí•  ë•Œ, **í•˜ë‚˜ì˜ ìë¦¬ì—ì„œ 1ì´ ë” í° ìˆ«ì**ê°€ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì¦‰, 9.11ì€ 9.9ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cec5a4",
   "metadata": {},
   "source": [
    "### 2ê°œì˜ ëª¨ë¸ì„ ì—°ë™í•œ ì½”ë“œë¥¼ Gradio ë¥¼ ì‚¬ìš©í•˜ì—¬ UIë¡œ ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f03026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pshcc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-2-xujkvS_f-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\pshcc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-2-xujkvS_f-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: asd\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 9\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: asd\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 9\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 1356\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: <think>\n",
      "Okay, let's tackle this query. The user provided a question \"asd\" and an inference process that's just a blank. My job is to create a Korean answer based on that. First, I need to understand what \"asd\" could mean. It's probably a typo or an abbreviation. Maybe it's \"asdasd\" or something else. But since the inference is empty, I need to figure out the best way to respond.\n",
      "\n",
      "The user's instructions say to analyze the given inference and create a structured answer in Korean. But since the inference is empty, I should focus on the question itself. \"asd\" is unclear, but maybe it's a test or a placeholder. I should consider common scenarios where \"asd\" might be used. Could it be a code, a term, or a random string?\n",
      "\n",
      "Since the user wants a response in Korean, I need to make sure the answer is natural and relevant. The key here is to address the question directly without mentioning the inference process. So, I'll explain that \"asd\" is unclear and suggest possible interpretations. Maybe it's a typo, a code, or a term. I'll list some possibilities and offer assistance to clarify. That way, the answer is helpful and meets the user's requirements without violating the guidelines.\n",
      "</think>\n",
      "\n",
      "\"asd\"ëŠ” ëª…í™•í•œ ì˜ë¯¸ê°€ ì—†ì–´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ í•´ì„ìœ¼ë¡œëŠ” í‚¤ë³´ë“œ ëˆ„ë¥´ê¸°(ASD í‚¤ ì¡°í•©), ì• ë‹ˆë©”ì´ì…˜ ë˜ëŠ” ì½”ë“œ ë“±ì˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ íŠ¹ì • ë§¥ë½ì—ì„œ í•´ë‹¹ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ ê²½ìš°, ì¶”ê°€ ì„¤ëª…ì„ ìš”ì²­í•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì¤˜\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 4535\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "Okay, so I'm trying to learn Python. I've heard a lot about it being useful for programming and data analysis, but I'm not entirely sure where to start. Let me think through how I can approach...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì¤˜\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 4535\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "Okay, so I'm trying to learn Python. I've heard a lot about it being useful for programming and data analysis, but I'm not entirely sure where to start. Let me think through how I can approach...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 673\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: <think>\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ë©° ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ì„ ì§€ì›í•˜ëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´ë¡œ, í”„ë¡œê·¸ë˜ë°, ë°ì´í„° ë¶„ì„, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ ì‹œì‘ì€ ë³€ìˆ˜ ì„ ì–¸ê³¼ ë°ì´í„° íƒ€ì…ë¶€í„° ì‹œì‘í•˜ë˜, í•¨ìˆ˜, ì¡°ê±´ë¬¸, ë£¨í”„, ì˜ˆì™¸ ì²˜ë¦¬, ë°ì´í„° êµ¬ì¡° ë“±ì€ í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë³€ìˆ˜ëŠ” ê°’ ì €ì¥ì„ ìœ„í•´ ì‚¬ìš©í•˜ë©°, íƒ€ì…(ì •ìˆ˜, ë¬¸ìì—´, ë¶€ìš¸ê°’ ë“±)ì„ ëª…í™•íˆ ì§€ì •í•  ìˆ˜ ìˆì–´ ì½”ë“œì˜ ê°€ë…ì„±ì´ ë†’ìŠµë‹ˆë‹¤. í•¨ìˆ˜ëŠ” ì½”ë“œì˜ ì¬ì‚¬ìš©ì„±ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ë˜ë©°, íŒŒì´ì¬ì—ì„œëŠ” ëª¨ë“ˆì„ í†µí•´ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: `math`, `random`)ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¡°ê±´ë¬¸ê³¼ ë°˜ë³µë¬¸ì€ í”„ë¡œê·¸ë¨ì˜ ë¡œì§ì„ êµ¬í˜„í•˜ëŠ” ë° í•„ìˆ˜ì ì´ë©°, `if-elif-else`ì™€ `for`, `while`ì„ í†µí•´ ë‹¤ì–‘í•œ ë¡œì§ì„ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì™¸ ì²˜ë¦¬(`try-except`)ëŠ” ì½”ë“œì˜ ì•ˆì •ì„±ì„ ë†’ì´ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë”•ì…”ë„ˆë¦¬, ì„¸íŠ¸)ëŠ” ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤˜ìš”. í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ë°˜í™˜ê°’ì€ ì½”ë“œì˜ ì¬ì‚¬ìš©ì„±ì„ ë†’ì´ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. í”„ë¡œê·¸ë˜ë°ì„ ì‹œì‘í•˜ëŠ” ê²ƒì€ ë‹¨ìˆœí•œ ê³¼ì •ì´ì§€ë§Œ, ê¸°ë³¸ì ì¸ ê°œë…ì„ ìµíˆê³  ì‹¤ë¬´ì—ì„œì˜ í™œìš©ì„ ëŠ˜ë ¤ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. íŒŒì´ì¬ì€ í•™ìŠµì´ ì‰¬ìš°ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ë„ í•™ìŠµ motivationì´ ë©ë‹ˆë‹¤. ğŸŒŸ\n",
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: 9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 468\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: 9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 468\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 671\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: <think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. The explanation given is that both start with 9, so they align the decimals to 9.90 and 9.11. Then compare each digit: tenths are both 9, but hundredths are 0 vs 1. Since 1 is bigger, 9.11 is larger. So the answer is 9.11.\n",
      "\n",
      "I need to present this in Korean, keeping it natural. Make sure to mention the alignment of decimals and the comparison of the hundredths place. Avoid the process steps but just the conclusion. Check for clarity and correctness.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” 9.11ì…ë‹ˆë‹¤. ë‘ ìˆ˜ëŠ” ì •ìˆ˜ë¶€ê°€ ê°™ìœ¼ë¯€ë¡œ ì†Œìˆ˜ë¶€ë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤. 9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¿”ë‘ë©´, 9.11ì´ 9.90ë³´ë‹¤ ì†Œìˆ˜ì  ì•„ë˜ì˜ 1ì´ ë” í° ìˆ˜ë¡œ, ê²°êµ­ 9.11ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Jupyter ë…¸íŠ¸ë¶ í˜¸í™˜)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter í™˜ê²½ì—ì„œëŠ” reconfigure ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter ë…¸íŠ¸ë¶ì´ë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” íŒ¨ìŠ¤\n",
    "    pass\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ê³¼ ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰\n",
    "# - reasoning_model: ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë‚®ìŒ, ì •í™•í•œ ë¶„ì„ìš©)\n",
    "# - generation_model: ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë†’ìŒ, ì°½ì˜ì  ì‘ë‹µìš©)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    #model=\"qwen2.5:1.5b\", \n",
    "    model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ìƒíƒœ(State) ì •ì˜: ê·¸ë˜í”„ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°\n",
    "class State(TypedDict):\n",
    "    question: str   # ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    thinking: str   # ì¶”ë¡  ê²°ê³¼\n",
    "    answer: str     # ìµœì¢… ë‹µë³€\n",
    "\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"ì§ˆë¬¸: {question}\n",
    "        \n",
    "        ì¶”ë¡  ê³¼ì •: {thinking}\n",
    "        \n",
    "        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"ë¬¸ìì—´ì´ UTF-8ë¡œ ì œëŒ€ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # ë¬¸ìì—´ì´ì§€ë§Œ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì •ë¦¬\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] ì…ë ¥ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] ì§ˆë¬¸ íƒ€ì…: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5ë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜: ìƒíƒœ(State) ê°„ì˜ íë¦„ì„ ì •ì˜\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    #launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394d68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-2-xujkvS_f-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
